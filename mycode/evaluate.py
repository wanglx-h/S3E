# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# import os
# import time
# import json
#
# from generate_trapdoor import generate_trapdoor
# from secure_search import (
#     secure_search,
#     get_cluster_docs,
#     merge_and_decrypt_docs,
# )
#
# # ===================== é…ç½® =====================
# # DATASETS = ["enron_1k", "enron_5k", "enron_10k"]
# DATASETS = ["wiki_1k", "wiki_5k", "wiki_10k"]
#
# # QUERIES = [
# #     # "What meetings are scheduled?",
# #     # "Tell me about energy trading",
# #     # "What contracts were discussed?",
# #     # "What are the price forecasts?",
# #     # "What reports need analysis?",
# #     # "What projects are in development?",
# #     # "What companies are involved?",
# #     "What emails need attention?",
# #     "What conference calls are planned?",
# #     # "What financial information is available?",
# #     # "Emails about SEC strategy meetings",
# #     # "Messages mentioning building access or badges",
# #     # "HR newsletters on labor or employment policy",
# #     # "Forwards with BNA Daily Labor Report content",
# #     # "Memos on minimum wage or unemployment issues",
# #     # "Emails discussing union negotiations or wage increases",
# #     "Messages about post-9/11 employment impacts",
# #     # "Notes on federal worker discrimination or whistleblower cases",
# #     # "Emails that list multiple labor news headlines",
# #     # "Messages sharing external news links with login info",
# #     "Internal calendar or on-call notification emails",
# #     "Emails between facilities or admin staff about office locations",
# #     # "Messages referencing ILO or international labor standards",
# #     # "Forwards about appointments to U.S. labor-related posts",
# #     # "Emails on benefit or donation program changes",
# #     # "Threads with multiple HR recipients in one blast",
# #     # "Messages mentioning airport security or related legislation",
# #     # "Emails summarizing congressional labor actions",
# #     # "Messages about court rulings on workplace drug testing",
# #     # "Long digest-style labor and employment updates",
# # ]
# QUERIES = [
#     # "What is the history of artificial intelligence?",
#     # "Tell me about the structure of the human brain.",
#     # "What are the major events of World War II?",
#     # "Explain the theory of evolution by Charles Darwin.",
#     # "What are the moons of Jupiter?",
#     # "Describe the process of photosynthesis.",
#     # "Who discovered gravity?",
#     # "What are the causes of climate change?",
#     "Explain quantum mechanics basics.",
#     # "Tell me about the culture of ancient Egypt.",
#     # "April month overview in the Gregorian calendar",
#     # "Etymology or origin of the name April",
#     # "April holidays and observances worldwide",
#     # "Seasonal description of April in both hemispheres",
#     # "Movable Christian feasts that fall in April",
#     "Sayings or phrases about April weather",
#     # "Historical events that happened in April",
#     # "April cultural festivals in Europe or Asia",
#     "Sports or major events usually held in April",
#     # "August month overview and calendar facts",
#     # "Etymology or origin of the name August",
#     # "August national or religious holidays",
#     # "August historical events in the 20th century",
#     # "Definition of art as human creative activity",
#     "Categories of art such as visual or performing",
#     # "Discussion of art versus design",
#     # "Short history outline of art across eras",
#     # "Examples of everyday objects treated as art",
#     # "Comparison of April seasons across hemispheres",
#     # "August cultural festivals and public holidays"
# ]
#
# RESULTS_DIR = r"../outputs/plain_results"
# OUTPUT_DIR = r"../outputs/eval"
# os.makedirs(OUTPUT_DIR, exist_ok=True)
#
#
# # ===================== å·¥å…·å‡½æ•° =====================
# def load_plain_results(dataset):
#     path = os.path.join(RESULTS_DIR, f"{dataset}.json")
#     if not os.path.exists(path):
#         print(f"[WARN] æ˜æ–‡ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {path}")
#         return {}
#     with open(path, "r", encoding="utf-8") as f:
#         return json.load(f)
#
#
# def compute_accuracy(secure_ids, plain_ids):
#     if not plain_ids:
#         return 0.0
#     secure_set = set(str(s).strip() for s in secure_ids)
#     plain_set = set(str(p).strip() for p in plain_ids)
#     inter = len(secure_set & plain_set)
#     return inter / len(secure_set) if secure_set else 0.0
#
#
# # ===================== ä¸»æµç¨‹ =====================
# def run_experiment():
#     final_output = {}
#
#     for dataset in DATASETS:
#         print("\n" + "=" * 100)
#         print(f"ğŸ”¹ Evaluating dataset: {dataset}")
#         print("=" * 100)
#
#         plain_results = load_plain_results(dataset)
#         if not plain_results:
#             print(f"[WARN] è·³è¿‡ {dataset}ï¼Œæœªæ‰¾åˆ°æ˜æ–‡ç»“æœæ–‡ä»¶ã€‚")
#             continue
#         total_trap = 0.0
#         total_time = 0.0
#         total_acc = 0.0
#         valid_queries = 0
#         perfect_count = 0
#         per_query_results = {}
#
#         for query in QUERIES:
#             if query not in plain_results:
#                 print(f"[WARN] {query} æœªåœ¨æ˜æ–‡ç»“æœä¸­æ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")
#                 continue
#
#             print(f"\n[QUERY] {query}")
#
#             try:
#                 # 1) ç”Ÿæˆé™·é—¨
#                 t_trap = time.time()
#                 t1, t2, q_piece = generate_trapdoor(query, dataset)
#                 t_trap1 = time.time() - t_trap
#                 total_trap += t_trap1
#
#                 # 2) å®‰å…¨æœç´¢
#                 t_start = time.time()
#                 best_cluster, best_piece, part_a_hex, part_b_hex = secure_search(
#                     t1, t2, dataset, q_piece=q_piece, debug=False
#                 )
#                 t_cost = time.time() - t_start
#                 total_time += t_cost
#
#                 # 3) åˆå¹¶å¹¶è§£å¯†æ–‡æ¡£é›†åˆ
#                 #    ä¼˜å…ˆç”¨ secure_search é‡Œçš„å®˜æ–¹è§£å¯†é€»è¾‘ï¼Œå¤±è´¥å†å›é€€æ˜æ–‡
#                 if part_a_hex or part_b_hex:
#                     try:
#                         secure_ids = merge_and_decrypt_docs(
#                             dataset,
#                             part_a_hex or "",
#                             part_b_hex or "",
#                         )
#                     except Exception as dec_e:
#                         print(f"[WARN] è§£å¯†æ–‡æ¡£é›†åˆå¤±è´¥ï¼Œä½¿ç”¨æ˜æ–‡å›é€€: {dec_e}")
#                         secure_ids = get_cluster_docs(best_cluster, dataset)
#                 else:
#                     secure_ids = get_cluster_docs(best_cluster, dataset)
#
#                 # 4) æ˜æ–‡æœŸæœ›
#                 plain_top_docs = plain_results[query]
#                 plain_ids = [str(d["id"]).strip() for d in plain_top_docs]
#
#                 # 5) å‡†ç¡®ç‡
#                 acc = compute_accuracy(secure_ids, plain_ids)
#                 total_acc += acc
#                 valid_queries += 1
#
#                 is_perfect = acc >= 0.999999
#                 if is_perfect:
#                     perfect_count += 1
#
#                 print(
#                     f" -> æœç´¢è€—æ—¶: {t_cost:.3f}s | q_piece: {q_piece} | "
#                     f"best_piece: {best_piece} | å‡†ç¡®ç‡: {acc:.3f}"
#                 )
#
#                 per_query_results[query] = {
#                     "secure_ids": [str(x).strip() for x in secure_ids],
#                     "plain_ids": plain_ids,
#                     "time": t_cost,
#                     "accuracy": acc,
#                     "q_piece": q_piece,
#                     "best_piece": best_piece,
#                     "cluster": best_cluster,
#                     "has_cipher_parts": bool(part_a_hex or part_b_hex),
#                 }
#
#             except KeyboardInterrupt:
#                 raise
#             except Exception as e:
#                 print(f"[ERROR] æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")
#         avg_trap = total_trap / valid_queries if valid_queries > 0 else 0.0
#         avg_time = total_time / valid_queries if valid_queries > 0 else 0.0
#         avg_acc = total_acc / valid_queries if valid_queries > 0 else 0.0
#         perfect_ratio = (perfect_count / valid_queries) if valid_queries > 0 else 0.0
#
#         print(f"\nâœ… æ•°æ®é›† {dataset} å¹³å‡æœç´¢æ—¶é—´: {avg_time:.3f}s"f", å¹³å‡å‡†ç¡®ç‡: {avg_acc:.3f}, å®Œå…¨æ­£ç¡®æŸ¥è¯¢å æ¯”: {perfect_ratio:.3f}")
#         print(f"\n å¹³å‡é™·é—¨ç”Ÿæˆæ—¶é—´ï¼š{avg_trap:.3f}s")
#         final_output[dataset] = {
#             "avg_time": avg_time,
#             "avg_acc": avg_acc,
#             "perfect_query_ratio": perfect_ratio,
#             "num_queries": valid_queries,
#             "queries": per_query_results,
#         }
#
#     out_path = os.path.join(OUTPUT_DIR, "wiki_f2_k20.json")
#     with open(out_path, "w", encoding="utf-8") as f:
#         json.dump(final_output, f, ensure_ascii=False, indent=2)
#
#     print("\n=== å®éªŒå®Œæˆ âœ… ===")
#     print(f"æ‰€æœ‰ç»“æœå·²å†™å…¥: {out_path}")
#
#
# if __name__ == "__main__":
#     run_experiment()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import time
import json

from generate_trapdoor import generate_trapdoor
from secure_search import (
    secure_search,
    get_cluster_docs,
    merge_and_decrypt_docs,
)

# ===================== é…ç½® =====================

# åªå¯¹ MS MARCO ä¸‰ä¸ªå­é›†åšå®éªŒ
DATASETS = ["msmarco_1k", "msmarco_5k", "msmarco_10k"]

# 15 æ¡æŸ¥è¯¢è¯­å¥ï¼ˆä¸ä½ ç¦»çº¿/åœ¨çº¿å®éªŒä¿æŒä¸€è‡´ï¼‰
QUERIES = [
    # "How do you use the Stefan-Boltzmann law to calculate the radius of a star such as Rigel from its luminosity and surface temperature?",
    # "What developmental milestones and typical behaviors should you expect from an 8 year old child at home and at school?",
    # "What are the symptoms of a head lice infestation and how can you check for lice, eggs, and nits on a child's scalp?",
    "What special features does the Burj Khalifa in Dubai have and why was it renamed from Burj Dubai?",
    "What kinds of homes and land are for sale near La Grange, California, and what are their typical sizes and prices?",
    "What are the main characteristics, temperament, and exercise needs of the Dogo Argentino dog breed?",
    # "How are custom safety nets used in industry and what kinds of clients and applications does a company like US Netting serve?",
    "What are effective ways to remove weeds from a garden and prevent them from coming back?",
    # "How common is urinary incontinence in the United States, what can cause it, and is it just a normal part of aging?",
    "How did President Franklin D. Roosevelt prepare the United States for World War II before Pearl Harbor while the country was still isolationist?",
    # "If you have multiple sclerosis and difficulty swallowing pills, is it safe to crush Valium and other medications to make them easier to swallow?",
    # "What strategies can help you get better results when dealing with customer service representatives at cable companies or airlines?",
    # "In Spanish, what does the word 'machacado' mean and how is the verb 'machacar' used in different contexts?",
    "When building a concrete path, how should you design and support plywood formwork so that it is strong enough and keeps the concrete in place?",
    "Why do people join political parties, and which political party did U.S. presidents Woodrow Wilson and Herbert Hoover belong to?",
]

# æ˜æ–‡ baseline ç»“æœç›®å½•ï¼ˆä¸å‰é¢è„šæœ¬ä¸€è‡´ï¼‰
RESULTS_DIR = "/root/siton-tmp/outputs/plain_results"

# è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
OUTPUT_DIR = "/root/siton-tmp/outputs/eval"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ===================== å·¥å…·å‡½æ•° =====================

def load_plain_results(dataset):
    path = os.path.join(RESULTS_DIR, f"{dataset}.json")
    if not os.path.exists(path):
        print(f"[WARN] æ˜æ–‡ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def compute_accuracy(secure_ids, plain_ids):
    if not plain_ids:
        return 0.0
    secure_set = set(str(s).strip() for s in secure_ids)
    plain_set = set(str(p).strip() for p in plain_ids)
    inter = len(secure_set & plain_set)
    return inter / len(secure_set) if secure_set else 0.0


# ===================== ä¸»æµç¨‹ =====================

def run_experiment():
    final_output = {}

    for dataset in DATASETS:
        print("\n" + "=" * 100)
        print(f"ğŸ”¹ Evaluating dataset: {dataset}")
        print("=" * 100)

        plain_results = load_plain_results(dataset)
        if not plain_results:
            print(f"[WARN] è·³è¿‡ {dataset}ï¼Œæœªæ‰¾åˆ°æ˜æ–‡ç»“æœæ–‡ä»¶ã€‚")
            continue

        total_trap = 0.0
        total_time = 0.0
        total_acc = 0.0
        valid_queries = 0
        perfect_count = 0
        per_query_results = {}

        for query in QUERIES:
            if query not in plain_results:
                print(f"[WARN] {query} æœªåœ¨æ˜æ–‡ç»“æœä¸­æ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")
                continue

            print(f"\n[QUERY] {query}")

            try:
                # 1) ç”Ÿæˆé™·é—¨ï¼ˆå®¢æˆ·ç«¯æŸ¥è¯¢æ„å»ºï¼‰
                t_trap = time.time()
                t1, t2, q_piece = generate_trapdoor(query, dataset)
                t_trap1 = time.time() - t_trap
                total_trap += t_trap1

                # 2) å®‰å…¨æœç´¢ï¼ˆæœåŠ¡å™¨ç«¯ç§æœ‰è®¡ç®—ï¼‰
                t_start = time.time()
                best_cluster, best_piece, part_a_hex, part_b_hex = secure_search(
                    t1, t2, dataset, q_piece=q_piece, debug=False
                )
                t_cost = time.time() - t_start
                total_time += t_cost

                # 3) åˆå¹¶å¹¶è§£å¯†æ–‡æ¡£é›†åˆ
                #    ä¼˜å…ˆç”¨ secure_search çš„è§£å¯†é€»è¾‘ï¼Œå¤±è´¥å†å›é€€æ˜æ–‡ç°‡æ–‡æ¡£
                if part_a_hex or part_b_hex:
                    try:
                        secure_ids = merge_and_decrypt_docs(
                            dataset,
                            part_a_hex or "",
                            part_b_hex or "",
                        )
                    except Exception as dec_e:
                        print(f"[WARN] è§£å¯†æ–‡æ¡£é›†åˆå¤±è´¥ï¼Œä½¿ç”¨æ˜æ–‡å›é€€: {dec_e}")
                        secure_ids = get_cluster_docs(best_cluster, dataset)
                else:
                    secure_ids = get_cluster_docs(best_cluster, dataset)

                # 4) æ˜æ–‡åŸºçº¿ â€œæ­£ç¡®ç­”æ¡ˆâ€
                plain_top_docs = plain_results[query]
                plain_ids = [str(d["id"]).strip() for d in plain_top_docs]

                # 5) å‡†ç¡®ç‡ï¼ˆsecure ç»“æœ vs æ˜æ–‡ç»“æœï¼‰
                acc = compute_accuracy(secure_ids, plain_ids)
                total_acc += acc
                valid_queries += 1

                is_perfect = acc >= 0.999999
                if is_perfect:
                    perfect_count += 1

                print(
                    f" -> æœç´¢è€—æ—¶: {t_cost:.3f}s | q_piece: {q_piece} | "
                    f"best_piece: {best_piece} | å‡†ç¡®ç‡: {acc:.3f}"
                )
                print("  secure_ids(sample):", [str(x) for x in secure_ids[:5]])
                print("  plain_ids(sample) :", plain_ids[:5])

                per_query_results[query] = {
                    "secure_ids": [str(x).strip() for x in secure_ids],
                    "plain_ids": plain_ids,
                    "time": t_cost,
                    "accuracy": acc,
                    "q_piece": q_piece,
                    "best_piece": best_piece,
                    "cluster": best_cluster,
                    "has_cipher_parts": bool(part_a_hex or part_b_hex),
                    "trapdoor_time": t_trap1,
                }

            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"[ERROR] æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")

        avg_trap = total_trap / valid_queries if valid_queries > 0 else 0.0
        avg_time = total_time / valid_queries if valid_queries > 0 else 0.0
        avg_acc = total_acc / valid_queries if valid_queries > 0 else 0.0
        perfect_ratio = (perfect_count / valid_queries) if valid_queries > 0 else 0.0

        print(
            f"\nâœ… æ•°æ®é›† {dataset} å¹³å‡æœç´¢æ—¶é—´: {avg_time:.3f}s"
            f", å¹³å‡å‡†ç¡®ç‡: {avg_acc:.3f}, å®Œå…¨æ­£ç¡®æŸ¥è¯¢å æ¯”: {perfect_ratio:.3f}"
        )
        print(f"\n å¹³å‡é™·é—¨ç”Ÿæˆæ—¶é—´ï¼š{avg_trap:.3f}s")

        final_output[dataset] = {
            "avg_time": avg_time,
            "avg_acc": avg_acc,
            "perfect_query_ratio": perfect_ratio,
            "avg_trapdoor_time": avg_trap,
            "num_queries": valid_queries,
            "queries": per_query_results,
        }

    # è¾“å‡ºæ–‡ä»¶åæ”¹ä¸º MS MARCO ç‰ˆæœ¬
    out_path = os.path.join(OUTPUT_DIR, "msmarco_f3_k20.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)

    print("\n=== å®éªŒå®Œæˆ âœ… ===")
    print(f"æ‰€æœ‰ç»“æœå·²å†™å…¥: {out_path}")


if __name__ == "__main__":
    run_experiment()
