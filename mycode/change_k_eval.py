#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
evaluate_enron_k.py
æŒ‰å½“å‰åŒæœåŠ¡å™¨ + AES-GCM æ‹†åˆ†çš„æµç¨‹åšè¯„ä¼°

åœ¨æ–°å»ºçš„ä¸‰ä¸ªç´¢å¼•ç»“æ„ï¼ˆk=10,20,40ï¼‰ä¸‹åˆ†åˆ«è¿›è¡Œå®éªŒï¼Œ
å¹¶ä¸ /root/siton-tmp/outputs/plain_results_k ä¸‹çš„æ˜æ–‡ç»“æœè®¡ç®—å‡†ç¡®ç‡ã€‚
"""

import os
import time
import json
import shutil

from generate_trapdoor import generate_trapdoor
from secure_search import (
    secure_search,
    get_cluster_docs,
    merge_and_decrypt_docs,
)

# ===================== è·¯å¾„é…ç½® =====================
CANON_INDEX_DIR = "/root/siton-tmp/outputs/index"

# æˆ‘ä»¬æ–°å»ºçš„ k åˆ†åˆ«ç´¢å¼•å­˜æ”¾ä½ç½®
K_INDEX_ROOT = "/root/siton-tmp/outputs/index_k"

# æ˜æ–‡ baseline ç»“æœç›®å½•
RESULTS_DIR = r"/root/siton-tmp/outputs/plain_results_k"

# è¯„ä¼°è¾“å‡ºç›®å½•
OUTPUT_DIR = r"../outputs/eval_k"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===================== æ•°æ®é›† & æŸ¥è¯¢ =====================

DATASETS = ["enron_5k"]

QUERIES = [
    # "What meetings are scheduled?",
    "Tell me about energy trading",
    # "What contracts were discussed?",
    # "What are the price forecasts?",
    # "What reports need analysis?",
    # "What projects are in development?",
    # "What companies are involved?",
    "What emails need attention?",
    "What conference calls are planned?",
    # "What financial information is available?",
    # "Emails about SEC strategy meetings",
    # "Messages mentioning building access or badges",
    # "HR newsletters on labor or employment policy",
    # "Forwards with BNA Daily Labor Report content",
    # "Memos on minimum wage or unemployment issues",
    # "Emails discussing union negotiations or wage increases",
    "Messages about post-9/11 employment impacts",
    # "Notes on federal worker discrimination or whistleblower cases",
    # "Emails that list multiple labor news headlines",
    # "Messages sharing external news links with login info",
    "Internal calendar or on-call notification emails",
    "Emails between facilities or admin staff about office locations",
    # "Messages referencing ILO or international labor standards",
    # "Forwards about appointments to U.S. labor-related posts",
    # "Emails on benefit or donation program changes",
    # "Threads with multiple HR recipients in one blast",
    # "Messages mentioning airport security or related legislation",
    # "Emails summarizing congressional labor actions",
    # "Messages about court rulings on workplace drug testing",
    # "Long digest-style labor and employment updates",
]


# ===================== å·¥å…·å‡½æ•° =====================

def load_plain_results(dataset, k_value):
    """
    ä» plain_results_k ç›®å½•åŠ è½½å¯¹åº” k çš„æ˜æ–‡ç»“æœã€‚
    æ–‡ä»¶å: enron_5k_k10.json / enron_5k_k20.json / enron_5k_k40.json
    """
    filename = f"{dataset}_k{k_value}.json"
    path = os.path.join(RESULTS_DIR, filename)
    if not os.path.exists(path):
        print(f"[WARN] æ˜æ–‡ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def compute_accuracy(secure_ids, plain_ids):
    if not plain_ids:
        return 0.0
    secure_set = set(str(s).strip() for s in secure_ids)
    plain_set = set(str(p).strip() for p in plain_ids)
    inter = len(secure_set & plain_set)
    return inter / len(secure_set) if secure_set else 0.0


def switch_index_for_k(dataset, k_value):
    src = os.path.join(K_INDEX_ROOT, f"k{k_value}", dataset)
    dst_root = CANON_INDEX_DIR
    dst = os.path.join(dst_root, dataset)

    if not os.path.isdir(src):
        raise FileNotFoundError(f"ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {src}")

    os.makedirs(dst_root, exist_ok=True)

    # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ æ‰ï¼ˆå¯èƒ½æ˜¯ç›®å½•æˆ–è€…ç¬¦å·é“¾æ¥ï¼‰
    if os.path.islink(dst) or os.path.exists(dst):
        if os.path.islink(dst):
            os.unlink(dst)
        elif os.path.isdir(dst):
            shutil.rmtree(dst)
        else:
            os.remove(dst)

    # ä¼˜å…ˆåˆ›å»ºç¬¦å·é“¾æ¥ï¼›è‹¥å¤±è´¥ï¼Œåˆ™å¤åˆ¶ç›®å½•
    try:
        os.symlink(src, dst)
        print(f"[INFO] å·²å°† {dst} è½¯é“¾æ¥åˆ° {src}")
    except (OSError, NotImplementedError) as e:
        print(f"[WARN] åˆ›å»ºè½¯é“¾æ¥å¤±è´¥ ({e})ï¼Œæ”¹ä¸ºå¤åˆ¶ç›®å½•")
        shutil.copytree(src, dst)
        print(f"[INFO] å·²å°† {src} å¤åˆ¶åˆ° {dst}")


# ===================== ä¸»æµç¨‹ =====================

def run_experiment():
    # ä¾æ¬¡å¯¹ k=10,20,40 åšä¸‰ç»„å®éªŒ
    for k_value in [10, 20, 40]:
        final_output = {}

        for dataset in DATASETS:
            print("\n" + "=" * 100)
            print(f"ğŸ”¹ Evaluating dataset: {dataset} (k={k_value})")
            print("=" * 100)

            # 1) åˆ‡æ¢ indexï¼šè®©å¯†æ–‡ä»£ç ä½¿ç”¨ index_k/k{k}/{dataset}
            try:
                switch_index_for_k(dataset, k_value)
            except Exception as e:
                print(f"[ERROR] åˆ‡æ¢ç´¢å¼•å¤±è´¥: {e}")
                continue

            # 2) åŠ è½½å¯¹åº” k çš„æ˜æ–‡ç»“æœ
            plain_results = load_plain_results(dataset, k_value)
            if not plain_results:
                print(f"[WARN] è·³è¿‡ {dataset}ï¼Œæœªæ‰¾åˆ°å¯¹åº” k={k_value} çš„æ˜æ–‡ç»“æœæ–‡ä»¶ã€‚")
                continue

            total_trap = 0.0
            total_time = 0.0
            total_acc = 0.0
            valid_queries = 0
            perfect_count = 0
            per_query_results = {}

            for query in QUERIES:
                if query not in plain_results:
                    print(f"[WARN] {query} æœªåœ¨æ˜æ–‡ç»“æœä¸­æ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")
                    continue

                print(f"\n[QUERY] {query}")

                try:
                    # 3) ç”Ÿæˆé™·é—¨ï¼ˆå†…éƒ¨ä»ç„¶ç”¨ CANON_INDEX_DIRï¼‰
                    t_trap = time.time()
                    t1, t2, q_piece = generate_trapdoor(query, dataset)
                    t_trap1 = time.time() - t_trap
                    total_trap += t_trap1

                    # 4) å®‰å…¨æœç´¢
                    t_start = time.time()
                    best_cluster, best_piece, part_a_hex, part_b_hex = secure_search(
                        t1, t2, dataset, q_piece=q_piece, debug=False
                    )
                    t_cost = time.time() - t_start
                    total_time += t_cost

                    # 5) åˆå¹¶å¹¶è§£å¯†æ–‡æ¡£é›†åˆ
                    if part_a_hex or part_b_hex:
                        try:
                            secure_ids = merge_and_decrypt_docs(
                                dataset,
                                part_a_hex or "",
                                part_b_hex or "",
                            )
                        except Exception as dec_e:
                            print(f"[WARN] è§£å¯†æ–‡æ¡£é›†åˆå¤±è´¥ï¼Œä½¿ç”¨æ˜æ–‡å›é€€: {dec_e}")
                            secure_ids = get_cluster_docs(best_cluster, dataset)
                    else:
                        secure_ids = get_cluster_docs(best_cluster, dataset)

                    # 6) æ˜æ–‡æœŸæœ›
                    plain_top_docs = plain_results[query]
                    plain_ids = [str(d["id"]).strip() for d in plain_top_docs]

                    # 7) å‡†ç¡®ç‡
                    acc = compute_accuracy(secure_ids, plain_ids)
                    total_acc += acc
                    valid_queries += 1

                    is_perfect = acc >= 0.999999
                    if is_perfect:
                        perfect_count += 1

                    print(
                        f" -> æœç´¢è€—æ—¶: {t_cost:.3f}s | q_piece: {q_piece} | "
                        f"best_piece: {best_piece} | å‡†ç¡®ç‡: {acc:.3f}"
                    )

                    per_query_results[query] = {
                        "secure_ids": [str(x).strip() for x in secure_ids],
                        "plain_ids": plain_ids,
                        "time": t_cost,
                        "accuracy": acc,
                        "q_piece": q_piece,
                        "best_piece": best_piece,
                        "cluster": best_cluster,
                        "has_cipher_parts": bool(part_a_hex or part_b_hex),
                    }

                except KeyboardInterrupt:
                    raise
                except Exception as e:
                    print(f"[ERROR] æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")

            avg_trap = total_trap / valid_queries if valid_queries > 0 else 0.0
            avg_time = total_time / valid_queries if valid_queries > 0 else 0.0
            avg_acc = total_acc / valid_queries if valid_queries > 0 else 0.0
            perfect_ratio = (perfect_count / valid_queries) if valid_queries > 0 else 0.0

            print(
                f"\nâœ… æ•°æ®é›† {dataset} (k={k_value}) å¹³å‡æœç´¢æ—¶é—´: {avg_time:.3f}s"
                f", å¹³å‡å‡†ç¡®ç‡: {avg_acc:.3f}, å®Œå…¨æ­£ç¡®æŸ¥è¯¢å æ¯”: {perfect_ratio:.3f}"
            )
            print(f"\n å¹³å‡é™·é—¨ç”Ÿæˆæ—¶é—´ï¼š{avg_trap:.3f}s")

            final_output[dataset] = {
                "k": k_value,
                "avg_time": avg_time,
                "avg_acc": avg_acc,
                "perfect_query_ratio": perfect_ratio,
                "num_queries": valid_queries,
                "avg_trap_time": avg_trap,
                "queries": per_query_results,
            }

        # 8) ä¿å­˜å½“å‰ k çš„æ€»ç»“ç»“æœ
        out_filename = f"enron_f2_k{k_value}.json"
        out_path = os.path.join(OUTPUT_DIR, out_filename)
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)

        print("\n=== æœ¬è½®å®éªŒå®Œæˆ âœ… ===")
        print(f"ç»“æœå·²å†™å…¥: {out_path}")


if __name__ == "__main__":
    run_experiment()
